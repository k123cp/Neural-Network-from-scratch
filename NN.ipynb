{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31005 Assignment 2 - 13301953.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X-pFoMqLmPi"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJLfPtM82JU2"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6viYI8vA2LeP"
      },
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialisation\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights randomly and biases to 0\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "\n",
        "        # Remember input values for calculating partial derivative \n",
        "        # during backpropagation\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    # dvalues - gradient passsed from the activation function\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# Input \"layer\"\n",
        "class Layer_Input:\n",
        "\n",
        "    # Pass inputs on to the first hidden layer\n",
        "    def forward(self, inputs, training):\n",
        "        self.output = inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgnQt6By17H1"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "Includes the ReLU activationfunction used in the hidden layers and the Softmax activation function at the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu5TBgxX2Bgx"
      },
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "\n",
        "        # Remember input values for calculating partial derivative \n",
        "        # during backpropagation\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Make a copy of dvalues - derivatives from the next layer\n",
        "        # to modify them\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Derivative of ReLU is the same as that passed from the next layer \n",
        "        # if it is not negative\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "\n",
        "        # Remember input values for calculating partial derivative \n",
        "        # during backpropagation\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        # Subtract the largest of the inputs before doing exponentiation\n",
        "        # to prevent overflow/\"exploding\" values down the line\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
        "\n",
        "        # Normalize them for each sample \n",
        "        # to convert to a probability distribution\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "\n",
        "            # Calculate Jacobian matrix of the output and\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4yZ9t9O10A6"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj578VOY129E"
      },
      "source": [
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "            (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "            (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                         bias_momentums_corrected / \\\n",
        "                         (np.sqrt(bias_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsMCbOco1sW1"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcLeeQHz1uzb"
      },
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy:\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "\n",
        "        # Calculate sample losses in a batch\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss for the batch\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Add accumulated sum of losses and sample count\n",
        "        self.accumulated_sum += np.sum(sample_losses)\n",
        "        self.accumulated_count += len(sample_losses)\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # if sparse labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # If one-hot encoded labels - convert to sparse\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "\n",
        "        # Normalize gradient to make their sumâ€™s (calculated by optimizers)\n",
        "        # magnitude invariant to the number of samples\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8s5oQU1iPG"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZL8IAFJ1mfp"
      },
      "source": [
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical:\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        self.accumulated_sum += np.sum(comparisons)\n",
        "        self.accumulated_count += len(comparisons)\n",
        "\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        # print(predictions)\n",
        "        # print(y)\n",
        "        # print(predictions.shape)\n",
        "        # print(y.shape)\n",
        "        return predictions == y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptSWRJ_r1Jwq"
      },
      "source": [
        "## Neural Network Model Class\n",
        "\n",
        "\n",
        "A model class to make it easier to build and train individual models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHnsvSNm1Kuq"
      },
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create a list of network objects\n",
        "        self.layers = []\n",
        "        # Softmax classifier's output object\n",
        "        self.softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Add objects to the model\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # Set loss, optimizer and accuracy\n",
        "    def set(self, *, loss, optimizer, accuracy):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.accuracy = accuracy\n",
        "\n",
        "    # Finalize the model\n",
        "    def finalize(self):\n",
        "\n",
        "        # Create and set the input layer\n",
        "        self.input_layer = Layer_Input()\n",
        "\n",
        "        # Count all the objects\n",
        "        layer_count = len(self.layers)\n",
        "\n",
        "        # Initialize a list containing trainable layers:\n",
        "        self.trainable_layers = []\n",
        "\n",
        "        # Iterate the objects\n",
        "        for i in range(layer_count):\n",
        "\n",
        "            # If it's the first layer,\n",
        "            # the previous layer object is the input layer\n",
        "            if i == 0:\n",
        "                self.layers[i].prev = self.input_layer\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # All hidden layers between first and last\n",
        "            elif i < layer_count - 1:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # The last layer - the next object is the loss\n",
        "            # Also save a reference to the last object\n",
        "            # whose output is the model's output\n",
        "            else:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.loss\n",
        "                self.output_layer_activation = self.layers[i]\n",
        "\n",
        "            # If layer contains an attribute called \"weights\",\n",
        "            # it's a trainable layer -\n",
        "            # add it to the list of trainable layers\n",
        "            # Don't need to check for biases -\n",
        "            # checking for weights is enough\n",
        "            if hasattr(self.layers[i], 'weights'):\n",
        "                self.trainable_layers.append(self.layers[i])\n",
        "\n",
        "            # Update loss object with trainable layers\n",
        "            self.loss.remember_trainable_layers(\n",
        "                self.trainable_layers\n",
        "            )\n",
        "\n",
        "    # Train the model\n",
        "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
        "              print_every=1, validation_data=None):\n",
        "\n",
        "        # Initialize accuracy object\n",
        "        self.accuracy.init(y)\n",
        "\n",
        "        # Default value if batch size is not set\n",
        "        train_steps = 1\n",
        "\n",
        "        # If there is validation data passed,\n",
        "        # set default number of steps for validation as well\n",
        "        if validation_data is not None:\n",
        "            validation_steps = 1\n",
        "\n",
        "            # For better readability\n",
        "            X_val, y_val = validation_data\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            train_steps = len(X) // batch_size\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full batch\n",
        "            if train_steps * batch_size < len(X):\n",
        "                train_steps += 1\n",
        "\n",
        "            if validation_data is not None:\n",
        "                validation_steps = len(X_val) // batch_size\n",
        "                # Dividing rounds down. If there are some remaining\n",
        "                # data but nor full batch, this won't include it\n",
        "                # Add `1` to include this not full batch\n",
        "                if validation_steps * batch_size < len(X_val):\n",
        "                    validation_steps += 1\n",
        "\n",
        "        # Main training loop\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            # Print epoch number\n",
        "            print('==============================================')\n",
        "            print(f'epoch: {epoch}')\n",
        "\n",
        "            # Reset accumulated values in loss and accuracy objects\n",
        "            self.loss.new_pass()\n",
        "            self.accuracy.new_pass()\n",
        "\n",
        "            # Iterate over steps\n",
        "            for step in range(train_steps):\n",
        "\n",
        "                # If batch size is not set -\n",
        "                # train using one step and full dataset\n",
        "                if batch_size is None:\n",
        "                    batch_X = X\n",
        "                    batch_y = y\n",
        "\n",
        "                # Otherwise slice a batch\n",
        "                else:\n",
        "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "                # Perform the forward pass\n",
        "                output = self.forward(batch_X, training=True)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = self.loss.calculate(output, batch_y)\n",
        "\n",
        "                # Get predictions and calculate an accuracy\n",
        "                predictions = self.output_layer_activation.predictions(\n",
        "                                  output)\n",
        "                accuracy = self.accuracy.calculate(predictions,\n",
        "                                                   batch_y)\n",
        "\n",
        "                # Perform backward pass\n",
        "                self.backward(output, batch_y)\n",
        "\n",
        "                # Optimize (update parameters)\n",
        "                self.optimizer.pre_update_params()\n",
        "                for layer in self.trainable_layers:\n",
        "                    self.optimizer.update_params(layer)\n",
        "                self.optimizer.post_update_params()\n",
        "\n",
        "                # Print a summary\n",
        "                if not step % print_every or step == train_steps - 1:\n",
        "                    print(f'step: {step}, ' +\n",
        "                          f'acc: {accuracy:.3f}, ' +\n",
        "                          f'loss: {loss:.3f} ' +\n",
        "                          f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # Get and print epoch loss and accuracy\n",
        "            epoch_loss = self.loss.calculate_accumulated()\n",
        "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "            print(f'training, ' +\n",
        "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
        "                  f'loss: {epoch_loss:.3f} ' +\n",
        "                  f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # If there is the validation data\n",
        "            if validation_data is not None:\n",
        "\n",
        "                # Evaluate the model:\n",
        "                self.evaluate(*validation_data,\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # Performs forward pass\n",
        "    def forward(self, X, training):\n",
        "\n",
        "        # Call forward method on the input layer\n",
        "        # this will set the output property that\n",
        "        # the first layer in \"prev\" object is expecting\n",
        "        self.input_layer.forward(X, training)\n",
        "\n",
        "        # Call forward method of every object in a chain\n",
        "        # Pass output of the previous object as a parameter\n",
        "        for layer in self.layers:\n",
        "            layer.forward(layer.prev.output, training)\n",
        "\n",
        "        # \"layer\" is now the last object from the list,\n",
        "        # return its output\n",
        "        return layer.output\n",
        "\n",
        "    # Performs backward pass\n",
        "    def backward(self, output, y):\n",
        "\n",
        "        # First call backward method\n",
        "        # on the combined activation/loss\n",
        "        # this will set dinputs property\n",
        "        self.softmax_classifier_output.backward(output, y)\n",
        "\n",
        "        # Since backward method of the last layer\n",
        "        # which is Softmax activation won't be called\n",
        "        # as we used combined activation/loss\n",
        "        # object, set dinputs in this object\n",
        "        self.layers[-1].dinputs = \\\n",
        "            self.softmax_classifier_output.dinputs\n",
        "\n",
        "        # Call backward method going through\n",
        "        # all the objects but last\n",
        "        # in reversed order passing dinputs as a parameter\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            layer.backward(layer.next.dinputs)\n",
        "\n",
        "        return\n",
        "    \n",
        "    # Evaluates the model using passed in dataset\n",
        "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
        "\n",
        "        # Default value if batch size is not being set\n",
        "        validation_steps = 1\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            validation_steps = len(X_val) // batch_size\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data, but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full minibatch\n",
        "            if validation_steps * batch_size < len(X_val):\n",
        "                validation_steps += 1\n",
        "\n",
        "        # Reset accumulated values in loss\n",
        "        # and accuracy objects\n",
        "        self.loss.new_pass()\n",
        "        self.accuracy.new_pass()\n",
        "\n",
        "        # Iterate over steps\n",
        "        for step in range(validation_steps):\n",
        "\n",
        "            # If batch size is not set -\n",
        "            # train using one step and full dataset\n",
        "            if batch_size is None:\n",
        "                batch_X = X_val\n",
        "                batch_y = y_val\n",
        "\n",
        "            # Otherwise slice a batch\n",
        "            else:\n",
        "                batch_X = X_val[\n",
        "                    step*batch_size:(step+1)*batch_size\n",
        "                ]\n",
        "                batch_y = y_val[\n",
        "                    step*batch_size:(step+1)*batch_size\n",
        "                ]\n",
        "\n",
        "            # Perform the forward pass\n",
        "            output = self.forward(batch_X, training=False)\n",
        "\n",
        "            # Calculate the loss\n",
        "            self.loss.calculate(output, batch_y)\n",
        "\n",
        "            # Get predictions and calculate an accuracy\n",
        "            predictions = self.output_layer_activation.predictions(\n",
        "                              output)\n",
        "            self.accuracy.calculate(predictions, batch_y)\n",
        "\n",
        "        # Get and print validation loss and accuracy\n",
        "        validation_loss = self.loss.calculate_accumulated()\n",
        "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "        # Print a summary\n",
        "        print(f'validation, ' +\n",
        "              f'acc: {validation_accuracy:.3f}, ' +\n",
        "              f'loss: {validation_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfkudGlm5p-M"
      },
      "source": [
        "## Training and Evaluation: Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bgdjTv4qVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4780476c-bc42-43a4-fd51-0e2d5d66a626"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# import the dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  \n",
        "y = iris.target\n",
        "\n",
        "# rescales the data set such that all feature values are in the range [-1, 1]\n",
        "scaler = MinMaxScaler([-1, 1])\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# split the training, evaluation and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=123)\n",
        "\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=456)\n",
        "\n",
        "# Instantiate the model\n",
        "modelx = Model()\n",
        "\n",
        "# Add layers\n",
        "modelx.add(Layer_Dense(X_train.shape[1], 128))\n",
        "modelx.add(Activation_ReLU())\n",
        "modelx.add(Layer_Dense(128, 128))\n",
        "modelx.add(Activation_ReLU())\n",
        "modelx.add(Layer_Dense(128, 3))\n",
        "modelx.add(Activation_Softmax())\n",
        "\n",
        "# Set loss, optimizer and accuracy objects\n",
        "modelx.set(\n",
        "    loss=Loss_CategoricalCrossentropy(),\n",
        "    optimizer=Optimizer_Adam(decay=1e-4),\n",
        "    accuracy=Accuracy_Categorical()\n",
        ")\n",
        "\n",
        "# Finalize the model\n",
        "modelx.finalize()\n",
        "\n",
        "# Train the model\n",
        "modelx.train(X_train, y_train, validation_data=(X_validate, y_validate),\n",
        "            epochs=150, batch_size=32, print_every=1)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluation on test set\")\n",
        "modelx.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================================\n",
            "epoch: 1\n",
            "step: 0, acc: 0.312, loss: 1.099 lr: 0.001\n",
            "step: 1, acc: 0.656, loss: 1.098 lr: 0.000999900009999\n",
            "step: 2, acc: 0.308, loss: 1.098 lr: 0.0009998000399920016\n",
            "training, acc: 0.433, loss: 1.098 lr: 0.0009998000399920016\n",
            "validation, acc: 0.200, loss: 1.098\n",
            "==============================================\n",
            "epoch: 2\n",
            "step: 0, acc: 0.375, loss: 1.097 lr: 0.000999700089973008\n",
            "step: 1, acc: 0.406, loss: 1.097 lr: 0.0009996001599360256\n",
            "step: 2, acc: 0.308, loss: 1.096 lr: 0.0009995002498750624\n",
            "training, acc: 0.367, loss: 1.097 lr: 0.0009995002498750624\n",
            "validation, acc: 0.467, loss: 1.096\n",
            "==============================================\n",
            "epoch: 3\n",
            "step: 0, acc: 0.625, loss: 1.095 lr: 0.0009994003597841295\n",
            "step: 1, acc: 0.719, loss: 1.093 lr: 0.0009993004896572402\n",
            "step: 2, acc: 0.615, loss: 1.092 lr: 0.0009992006394884093\n",
            "training, acc: 0.656, loss: 1.093 lr: 0.0009992006394884093\n",
            "validation, acc: 0.533, loss: 1.092\n",
            "==============================================\n",
            "epoch: 4\n",
            "step: 0, acc: 0.656, loss: 1.090 lr: 0.0009991008092716555\n",
            "step: 1, acc: 0.719, loss: 1.086 lr: 0.0009990009990009992\n",
            "step: 2, acc: 0.615, loss: 1.085 lr: 0.0009989012086704624\n",
            "training, acc: 0.667, loss: 1.087 lr: 0.0009989012086704624\n",
            "validation, acc: 0.533, loss: 1.084\n",
            "==============================================\n",
            "epoch: 5\n",
            "step: 0, acc: 0.656, loss: 1.082 lr: 0.000998801438274071\n",
            "step: 1, acc: 0.719, loss: 1.076 lr: 0.0009987016878058523\n",
            "step: 2, acc: 0.615, loss: 1.074 lr: 0.000998601957259836\n",
            "training, acc: 0.667, loss: 1.077 lr: 0.000998601957259836\n",
            "validation, acc: 0.533, loss: 1.072\n",
            "==============================================\n",
            "epoch: 6\n",
            "step: 0, acc: 0.656, loss: 1.069 lr: 0.0009985022466300548\n",
            "step: 1, acc: 0.719, loss: 1.059 lr: 0.000998402555910543\n",
            "step: 2, acc: 0.615, loss: 1.057 lr: 0.000998302885095338\n",
            "training, acc: 0.667, loss: 1.062 lr: 0.000998302885095338\n",
            "validation, acc: 0.533, loss: 1.054\n",
            "==============================================\n",
            "epoch: 7\n",
            "step: 0, acc: 0.656, loss: 1.050 lr: 0.0009982032341784787\n",
            "step: 1, acc: 0.719, loss: 1.034 lr: 0.0009981036031540074\n",
            "step: 2, acc: 0.615, loss: 1.032 lr: 0.000998003992015968\n",
            "training, acc: 0.667, loss: 1.039 lr: 0.000998003992015968\n",
            "validation, acc: 0.533, loss: 1.029\n",
            "==============================================\n",
            "epoch: 8\n",
            "step: 0, acc: 0.656, loss: 1.022 lr: 0.0009979044007584073\n",
            "step: 1, acc: 0.719, loss: 0.999 lr: 0.0009978048293753743\n",
            "step: 2, acc: 0.615, loss: 0.997 lr: 0.0009977052778609198\n",
            "training, acc: 0.667, loss: 1.007 lr: 0.0009977052778609198\n",
            "validation, acc: 0.533, loss: 0.995\n",
            "==============================================\n",
            "epoch: 9\n",
            "step: 0, acc: 0.656, loss: 0.986 lr: 0.0009976057462090984\n",
            "step: 1, acc: 0.719, loss: 0.953 lr: 0.0009975062344139652\n",
            "step: 2, acc: 0.615, loss: 0.952 lr: 0.0009974067424695793\n",
            "training, acc: 0.667, loss: 0.965 lr: 0.0009974067424695793\n",
            "validation, acc: 0.533, loss: 0.951\n",
            "==============================================\n",
            "epoch: 10\n",
            "step: 0, acc: 0.656, loss: 0.938 lr: 0.0009973072703700011\n",
            "step: 1, acc: 0.719, loss: 0.895 lr: 0.000997207818109294\n",
            "step: 2, acc: 0.615, loss: 0.897 lr: 0.0009971083856815236\n",
            "training, acc: 0.667, loss: 0.911 lr: 0.0009971083856815236\n",
            "validation, acc: 0.533, loss: 0.900\n",
            "==============================================\n",
            "epoch: 11\n",
            "step: 0, acc: 0.656, loss: 0.881 lr: 0.0009970089730807579\n",
            "step: 1, acc: 0.719, loss: 0.825 lr: 0.0009969095803010666\n",
            "step: 2, acc: 0.615, loss: 0.833 lr: 0.000996810207336523\n",
            "training, acc: 0.667, loss: 0.847 lr: 0.000996810207336523\n",
            "validation, acc: 0.533, loss: 0.843\n",
            "==============================================\n",
            "epoch: 12\n",
            "step: 0, acc: 0.656, loss: 0.815 lr: 0.000996710854181202\n",
            "step: 1, acc: 0.719, loss: 0.747 lr: 0.0009966115208291807\n",
            "step: 2, acc: 0.615, loss: 0.765 lr: 0.000996512207274539\n",
            "training, acc: 0.667, loss: 0.776 lr: 0.000996512207274539\n",
            "validation, acc: 0.533, loss: 0.787\n",
            "==============================================\n",
            "epoch: 13\n",
            "step: 0, acc: 0.656, loss: 0.746 lr: 0.0009964129135113591\n",
            "step: 1, acc: 0.719, loss: 0.666 lr: 0.0009963136395337252\n",
            "step: 2, acc: 0.615, loss: 0.698 lr: 0.0009962143853357243\n",
            "training, acc: 0.667, loss: 0.704 lr: 0.0009962143853357243\n",
            "validation, acc: 0.533, loss: 0.738\n",
            "==============================================\n",
            "epoch: 14\n",
            "step: 0, acc: 0.656, loss: 0.680 lr: 0.0009961151509114453\n",
            "step: 1, acc: 0.719, loss: 0.591 lr: 0.00099601593625498\n",
            "step: 2, acc: 0.615, loss: 0.640 lr: 0.0009959167413604224\n",
            "training, acc: 0.667, loss: 0.637 lr: 0.0009959167413604224\n",
            "validation, acc: 0.533, loss: 0.702\n",
            "==============================================\n",
            "epoch: 15\n",
            "step: 0, acc: 0.656, loss: 0.623 lr: 0.0009958175662218682\n",
            "step: 1, acc: 0.719, loss: 0.529 lr: 0.0009957184108334164\n",
            "step: 2, acc: 0.615, loss: 0.595 lr: 0.0009956192751891678\n",
            "training, acc: 0.667, loss: 0.581 lr: 0.0009956192751891678\n",
            "validation, acc: 0.533, loss: 0.680\n",
            "==============================================\n",
            "epoch: 16\n",
            "step: 0, acc: 0.656, loss: 0.578 lr: 0.0009955201592832257\n",
            "step: 1, acc: 0.719, loss: 0.481 lr: 0.0009954210631096954\n",
            "step: 2, acc: 0.615, loss: 0.560 lr: 0.0009953219866626855\n",
            "training, acc: 0.667, loss: 0.538 lr: 0.0009953219866626855\n",
            "validation, acc: 0.533, loss: 0.665\n",
            "==============================================\n",
            "epoch: 17\n",
            "step: 0, acc: 0.656, loss: 0.542 lr: 0.0009952229299363057\n",
            "step: 1, acc: 0.719, loss: 0.446 lr: 0.0009951238929246692\n",
            "step: 2, acc: 0.615, loss: 0.531 lr: 0.0009950248756218907\n",
            "training, acc: 0.667, loss: 0.505 lr: 0.0009950248756218907\n",
            "validation, acc: 0.533, loss: 0.649\n",
            "==============================================\n",
            "epoch: 18\n",
            "step: 0, acc: 0.719, loss: 0.513 lr: 0.0009949258780220871\n",
            "step: 1, acc: 0.781, loss: 0.418 lr: 0.000994826900119379\n",
            "step: 2, acc: 0.615, loss: 0.502 lr: 0.0009947279419078882\n",
            "training, acc: 0.711, loss: 0.476 lr: 0.0009947279419078882\n",
            "validation, acc: 0.567, loss: 0.624\n",
            "==============================================\n",
            "epoch: 19\n",
            "step: 0, acc: 0.750, loss: 0.485 lr: 0.0009946290033817386\n",
            "step: 1, acc: 0.781, loss: 0.392 lr: 0.000994530084535057\n",
            "step: 2, acc: 0.615, loss: 0.470 lr: 0.000994431185361973\n",
            "training, acc: 0.722, loss: 0.448 lr: 0.000994431185361973\n",
            "validation, acc: 0.633, loss: 0.590\n",
            "==============================================\n",
            "epoch: 20\n",
            "step: 0, acc: 0.750, loss: 0.456 lr: 0.0009943323058566173\n",
            "step: 1, acc: 0.844, loss: 0.367 lr: 0.0009942334460131238\n",
            "step: 2, acc: 0.654, loss: 0.435 lr: 0.0009941346058256288\n",
            "training, acc: 0.756, loss: 0.418 lr: 0.0009941346058256288\n",
            "validation, acc: 0.633, loss: 0.548\n",
            "==============================================\n",
            "epoch: 21\n",
            "step: 0, acc: 0.781, loss: 0.425 lr: 0.0009940357852882705\n",
            "step: 1, acc: 0.875, loss: 0.343 lr: 0.0009939369843951894\n",
            "step: 2, acc: 0.769, loss: 0.397 lr: 0.0009938382031405288\n",
            "training, acc: 0.811, loss: 0.388 lr: 0.0009938382031405288\n",
            "validation, acc: 0.667, loss: 0.504\n",
            "==============================================\n",
            "epoch: 22\n",
            "step: 0, acc: 0.844, loss: 0.393 lr: 0.0009937394415184338\n",
            "step: 1, acc: 0.906, loss: 0.320 lr: 0.0009936406995230524\n",
            "step: 2, acc: 0.923, loss: 0.359 lr: 0.0009935419771485345\n",
            "training, acc: 0.889, loss: 0.357 lr: 0.0009935419771485345\n",
            "validation, acc: 0.733, loss: 0.461\n",
            "==============================================\n",
            "epoch: 23\n",
            "step: 0, acc: 0.906, loss: 0.361 lr: 0.0009934432743890324\n",
            "step: 1, acc: 0.938, loss: 0.296 lr: 0.0009933445912387007\n",
            "step: 2, acc: 0.923, loss: 0.321 lr: 0.0009932459276916965\n",
            "training, acc: 0.922, loss: 0.327 lr: 0.0009932459276916965\n",
            "validation, acc: 0.800, loss: 0.421\n",
            "==============================================\n",
            "epoch: 24\n",
            "step: 0, acc: 0.875, loss: 0.330 lr: 0.000993147283742179\n",
            "step: 1, acc: 0.938, loss: 0.271 lr: 0.00099304865938431\n",
            "step: 2, acc: 1.000, loss: 0.285 lr: 0.000992950054612253\n",
            "training, acc: 0.933, loss: 0.296 lr: 0.000992950054612253\n",
            "validation, acc: 0.833, loss: 0.385\n",
            "==============================================\n",
            "epoch: 25\n",
            "step: 0, acc: 0.875, loss: 0.300 lr: 0.0009928514694201747\n",
            "step: 1, acc: 0.938, loss: 0.246 lr: 0.0009927529038022435\n",
            "step: 2, acc: 1.000, loss: 0.250 lr: 0.0009926543577526304\n",
            "training, acc: 0.933, loss: 0.266 lr: 0.0009926543577526304\n",
            "validation, acc: 0.833, loss: 0.356\n",
            "==============================================\n",
            "epoch: 26\n",
            "step: 0, acc: 0.938, loss: 0.271 lr: 0.0009925558312655087\n",
            "step: 1, acc: 0.938, loss: 0.220 lr: 0.0009924573243350536\n",
            "step: 2, acc: 1.000, loss: 0.218 lr: 0.0009923588369554431\n",
            "training, acc: 0.956, loss: 0.238 lr: 0.0009923588369554431\n",
            "validation, acc: 0.833, loss: 0.332\n",
            "==============================================\n",
            "epoch: 27\n",
            "step: 0, acc: 0.938, loss: 0.245 lr: 0.0009922603691208572\n",
            "step: 1, acc: 0.938, loss: 0.196 lr: 0.0009921619208254787\n",
            "step: 2, acc: 1.000, loss: 0.188 lr: 0.000992063492063492\n",
            "training, acc: 0.956, loss: 0.211 lr: 0.000992063492063492\n",
            "validation, acc: 0.867, loss: 0.310\n",
            "==============================================\n",
            "epoch: 28\n",
            "step: 0, acc: 0.938, loss: 0.222 lr: 0.0009919650828290846\n",
            "step: 1, acc: 0.938, loss: 0.174 lr: 0.0009918666931164452\n",
            "step: 2, acc: 1.000, loss: 0.162 lr: 0.000991768322919766\n",
            "training, acc: 0.956, loss: 0.187 lr: 0.000991768322919766\n",
            "validation, acc: 0.867, loss: 0.288\n",
            "==============================================\n",
            "epoch: 29\n",
            "step: 0, acc: 0.938, loss: 0.201 lr: 0.0009916699722332409\n",
            "step: 1, acc: 0.938, loss: 0.154 lr: 0.000991571641051066\n",
            "step: 2, acc: 1.000, loss: 0.138 lr: 0.00099147332936744\n",
            "training, acc: 0.956, loss: 0.166 lr: 0.00099147332936744\n",
            "validation, acc: 0.867, loss: 0.267\n",
            "==============================================\n",
            "epoch: 30\n",
            "step: 0, acc: 0.969, loss: 0.184 lr: 0.000991375037176564\n",
            "step: 1, acc: 0.938, loss: 0.138 lr: 0.0009912767644726407\n",
            "step: 2, acc: 1.000, loss: 0.119 lr: 0.0009911785112498763\n",
            "training, acc: 0.967, loss: 0.149 lr: 0.0009911785112498763\n",
            "validation, acc: 0.867, loss: 0.248\n",
            "==============================================\n",
            "epoch: 31\n",
            "step: 0, acc: 0.969, loss: 0.169 lr: 0.000991080277502478\n",
            "step: 1, acc: 0.938, loss: 0.123 lr: 0.0009909820632246556\n",
            "step: 2, acc: 1.000, loss: 0.102 lr: 0.0009908838684106222\n",
            "training, acc: 0.967, loss: 0.133 lr: 0.0009908838684106222\n",
            "validation, acc: 0.867, loss: 0.231\n",
            "==============================================\n",
            "epoch: 32\n",
            "step: 0, acc: 0.969, loss: 0.156 lr: 0.0009907856930545921\n",
            "step: 1, acc: 0.969, loss: 0.112 lr: 0.0009906875371507827\n",
            "step: 2, acc: 1.000, loss: 0.088 lr: 0.0009905894006934125\n",
            "training, acc: 0.978, loss: 0.121 lr: 0.0009905894006934125\n",
            "validation, acc: 0.867, loss: 0.218\n",
            "==============================================\n",
            "epoch: 33\n",
            "step: 0, acc: 0.969, loss: 0.145 lr: 0.0009904912836767037\n",
            "step: 1, acc: 0.969, loss: 0.103 lr: 0.0009903931860948795\n",
            "step: 2, acc: 1.000, loss: 0.076 lr: 0.0009902951079421667\n",
            "training, acc: 0.978, loss: 0.110 lr: 0.0009902951079421667\n",
            "validation, acc: 0.867, loss: 0.206\n",
            "==============================================\n",
            "epoch: 34\n",
            "step: 0, acc: 0.969, loss: 0.134 lr: 0.0009901970492127933\n",
            "step: 1, acc: 0.969, loss: 0.095 lr: 0.0009900990099009901\n",
            "step: 2, acc: 1.000, loss: 0.066 lr: 0.00099000099000099\n",
            "training, acc: 0.978, loss: 0.101 lr: 0.00099000099000099\n",
            "validation, acc: 0.867, loss: 0.196\n",
            "==============================================\n",
            "epoch: 35\n",
            "step: 0, acc: 0.969, loss: 0.125 lr: 0.0009899029895070284\n",
            "step: 1, acc: 0.969, loss: 0.089 lr: 0.0009898050084133426\n",
            "step: 2, acc: 1.000, loss: 0.058 lr: 0.0009897070467141727\n",
            "training, acc: 0.978, loss: 0.093 lr: 0.0009897070467141727\n",
            "validation, acc: 0.933, loss: 0.186\n",
            "==============================================\n",
            "epoch: 36\n",
            "step: 0, acc: 0.969, loss: 0.116 lr: 0.0009896091044037606\n",
            "step: 1, acc: 0.969, loss: 0.084 lr: 0.0009895111814763509\n",
            "step: 2, acc: 1.000, loss: 0.051 lr: 0.0009894132779261898\n",
            "training, acc: 0.978, loss: 0.086 lr: 0.0009894132779261898\n",
            "validation, acc: 0.933, loss: 0.175\n",
            "==============================================\n",
            "epoch: 37\n",
            "step: 0, acc: 0.969, loss: 0.108 lr: 0.0009893153937475267\n",
            "step: 1, acc: 0.969, loss: 0.080 lr: 0.0009892175289346128\n",
            "step: 2, acc: 1.000, loss: 0.046 lr: 0.0009891196834817015\n",
            "training, acc: 0.978, loss: 0.080 lr: 0.0009891196834817015\n",
            "validation, acc: 0.933, loss: 0.166\n",
            "==============================================\n",
            "epoch: 38\n",
            "step: 0, acc: 0.969, loss: 0.101 lr: 0.000989021857383048\n",
            "step: 1, acc: 0.969, loss: 0.076 lr: 0.0009889240506329115\n",
            "step: 2, acc: 1.000, loss: 0.041 lr: 0.0009888262632255513\n",
            "training, acc: 0.978, loss: 0.075 lr: 0.0009888262632255513\n",
            "validation, acc: 0.933, loss: 0.157\n",
            "==============================================\n",
            "epoch: 39\n",
            "step: 0, acc: 0.969, loss: 0.095 lr: 0.0009887284951552303\n",
            "step: 1, acc: 0.969, loss: 0.072 lr: 0.0009886307464162135\n",
            "step: 2, acc: 1.000, loss: 0.037 lr: 0.0009885330170027679\n",
            "training, acc: 0.978, loss: 0.070 lr: 0.0009885330170027679\n",
            "validation, acc: 0.933, loss: 0.149\n",
            "==============================================\n",
            "epoch: 40\n",
            "step: 0, acc: 0.969, loss: 0.089 lr: 0.0009884353069091628\n",
            "step: 1, acc: 0.969, loss: 0.069 lr: 0.0009883376161296698\n",
            "step: 2, acc: 1.000, loss: 0.033 lr: 0.000988239944658563\n",
            "training, acc: 0.978, loss: 0.066 lr: 0.000988239944658563\n",
            "validation, acc: 0.933, loss: 0.143\n",
            "==============================================\n",
            "epoch: 41\n",
            "step: 0, acc: 0.969, loss: 0.084 lr: 0.0009881422924901185\n",
            "step: 1, acc: 0.969, loss: 0.066 lr: 0.0009880446596186147\n",
            "step: 2, acc: 1.000, loss: 0.030 lr: 0.0009879470460383323\n",
            "training, acc: 0.978, loss: 0.062 lr: 0.0009879470460383323\n",
            "validation, acc: 0.933, loss: 0.138\n",
            "==============================================\n",
            "epoch: 42\n",
            "step: 0, acc: 0.969, loss: 0.079 lr: 0.0009878494517435543\n",
            "step: 1, acc: 0.969, loss: 0.063 lr: 0.0009877518767285659\n",
            "step: 2, acc: 1.000, loss: 0.027 lr: 0.0009876543209876543\n",
            "training, acc: 0.978, loss: 0.058 lr: 0.0009876543209876543\n",
            "validation, acc: 0.933, loss: 0.132\n",
            "==============================================\n",
            "epoch: 43\n",
            "step: 0, acc: 0.969, loss: 0.074 lr: 0.0009875567845151097\n",
            "step: 1, acc: 0.969, loss: 0.061 lr: 0.0009874592673052237\n",
            "step: 2, acc: 1.000, loss: 0.025 lr: 0.0009873617693522908\n",
            "training, acc: 0.978, loss: 0.055 lr: 0.0009873617693522908\n",
            "validation, acc: 0.933, loss: 0.127\n",
            "==============================================\n",
            "epoch: 44\n",
            "step: 0, acc: 0.969, loss: 0.070 lr: 0.0009872642906506074\n",
            "step: 1, acc: 0.969, loss: 0.059 lr: 0.000987166831194472\n",
            "step: 2, acc: 1.000, loss: 0.023 lr: 0.0009870693909781857\n",
            "training, acc: 0.978, loss: 0.053 lr: 0.0009870693909781857\n",
            "validation, acc: 0.933, loss: 0.123\n",
            "==============================================\n",
            "epoch: 45\n",
            "step: 0, acc: 0.969, loss: 0.067 lr: 0.000986971969996052\n",
            "step: 1, acc: 0.969, loss: 0.056 lr: 0.0009868745682423763\n",
            "step: 2, acc: 1.000, loss: 0.021 lr: 0.0009867771857114663\n",
            "training, acc: 0.978, loss: 0.050 lr: 0.0009867771857114663\n",
            "validation, acc: 0.933, loss: 0.119\n",
            "==============================================\n",
            "epoch: 46\n",
            "step: 0, acc: 1.000, loss: 0.064 lr: 0.0009866798223976318\n",
            "step: 1, acc: 0.969, loss: 0.054 lr: 0.0009865824782951855\n",
            "step: 2, acc: 1.000, loss: 0.020 lr: 0.0009864851533984414\n",
            "training, acc: 0.989, loss: 0.048 lr: 0.0009864851533984414\n",
            "validation, acc: 0.933, loss: 0.115\n",
            "==============================================\n",
            "epoch: 47\n",
            "step: 0, acc: 1.000, loss: 0.061 lr: 0.0009863878477017164\n",
            "step: 1, acc: 0.969, loss: 0.052 lr: 0.0009862905611993293\n",
            "step: 2, acc: 1.000, loss: 0.019 lr: 0.0009861932938856016\n",
            "training, acc: 0.989, loss: 0.046 lr: 0.0009861932938856016\n",
            "validation, acc: 0.933, loss: 0.112\n",
            "==============================================\n",
            "epoch: 48\n",
            "step: 0, acc: 1.000, loss: 0.059 lr: 0.0009860960457548566\n",
            "step: 1, acc: 0.969, loss: 0.050 lr: 0.00098599881680142\n",
            "step: 2, acc: 1.000, loss: 0.017 lr: 0.0009859016070196194\n",
            "training, acc: 0.989, loss: 0.044 lr: 0.0009859016070196194\n",
            "validation, acc: 0.933, loss: 0.109\n",
            "==============================================\n",
            "epoch: 49\n",
            "step: 0, acc: 1.000, loss: 0.057 lr: 0.0009858044164037854\n",
            "step: 1, acc: 0.969, loss: 0.049 lr: 0.0009857072449482504\n",
            "step: 2, acc: 1.000, loss: 0.016 lr: 0.0009856100926473488\n",
            "training, acc: 0.989, loss: 0.042 lr: 0.0009856100926473488\n",
            "validation, acc: 0.933, loss: 0.106\n",
            "==============================================\n",
            "epoch: 50\n",
            "step: 0, acc: 1.000, loss: 0.055 lr: 0.0009855129594954174\n",
            "step: 1, acc: 0.969, loss: 0.047 lr: 0.0009854158454867955\n",
            "step: 2, acc: 1.000, loss: 0.015 lr: 0.0009853187506158243\n",
            "training, acc: 0.989, loss: 0.041 lr: 0.0009853187506158243\n",
            "validation, acc: 0.933, loss: 0.104\n",
            "==============================================\n",
            "epoch: 51\n",
            "step: 0, acc: 1.000, loss: 0.053 lr: 0.0009852216748768474\n",
            "step: 1, acc: 0.969, loss: 0.045 lr: 0.0009851246182642104\n",
            "step: 2, acc: 1.000, loss: 0.014 lr: 0.0009850275807722615\n",
            "training, acc: 0.989, loss: 0.039 lr: 0.0009850275807722615\n",
            "validation, acc: 0.933, loss: 0.102\n",
            "==============================================\n",
            "epoch: 52\n",
            "step: 0, acc: 1.000, loss: 0.052 lr: 0.000984930562395351\n",
            "step: 1, acc: 0.969, loss: 0.044 lr: 0.0009848335631278313\n",
            "step: 2, acc: 1.000, loss: 0.013 lr: 0.0009847365829640572\n",
            "training, acc: 0.989, loss: 0.038 lr: 0.0009847365829640572\n",
            "validation, acc: 0.933, loss: 0.100\n",
            "==============================================\n",
            "epoch: 53\n",
            "step: 0, acc: 1.000, loss: 0.050 lr: 0.0009846396218983853\n",
            "step: 1, acc: 0.969, loss: 0.042 lr: 0.0009845426799251747\n",
            "step: 2, acc: 1.000, loss: 0.013 lr: 0.0009844457570387872\n",
            "training, acc: 0.989, loss: 0.037 lr: 0.0009844457570387872\n",
            "validation, acc: 0.933, loss: 0.099\n",
            "==============================================\n",
            "epoch: 54\n",
            "step: 0, acc: 1.000, loss: 0.049 lr: 0.0009843488532335861\n",
            "step: 1, acc: 1.000, loss: 0.040 lr: 0.000984251968503937\n",
            "step: 2, acc: 1.000, loss: 0.012 lr: 0.0009841551028442082\n",
            "training, acc: 1.000, loss: 0.035 lr: 0.0009841551028442082\n",
            "validation, acc: 0.933, loss: 0.097\n",
            "==============================================\n",
            "epoch: 55\n",
            "step: 0, acc: 1.000, loss: 0.048 lr: 0.00098405825624877\n",
            "step: 1, acc: 1.000, loss: 0.039 lr: 0.0009839614287119945\n",
            "step: 2, acc: 1.000, loss: 0.011 lr: 0.0009838646202282567\n",
            "training, acc: 1.000, loss: 0.034 lr: 0.0009838646202282567\n",
            "validation, acc: 0.933, loss: 0.096\n",
            "==============================================\n",
            "epoch: 56\n",
            "step: 0, acc: 1.000, loss: 0.047 lr: 0.0009837678307919333\n",
            "step: 1, acc: 1.000, loss: 0.038 lr: 0.0009836710603974032\n",
            "step: 2, acc: 1.000, loss: 0.011 lr: 0.000983574309039048\n",
            "training, acc: 1.000, loss: 0.033 lr: 0.000983574309039048\n",
            "validation, acc: 0.933, loss: 0.095\n",
            "==============================================\n",
            "epoch: 57\n",
            "step: 0, acc: 1.000, loss: 0.046 lr: 0.000983477576711251\n",
            "step: 1, acc: 1.000, loss: 0.036 lr: 0.0009833808634083982\n",
            "step: 2, acc: 1.000, loss: 0.010 lr: 0.0009832841691248771\n",
            "training, acc: 1.000, loss: 0.032 lr: 0.0009832841691248771\n",
            "validation, acc: 0.933, loss: 0.093\n",
            "==============================================\n",
            "epoch: 58\n",
            "step: 0, acc: 1.000, loss: 0.045 lr: 0.0009831874938550783\n",
            "step: 1, acc: 1.000, loss: 0.035 lr: 0.0009830908375933936\n",
            "step: 2, acc: 1.000, loss: 0.010 lr: 0.000982994200334218\n",
            "training, acc: 1.000, loss: 0.031 lr: 0.000982994200334218\n",
            "validation, acc: 0.933, loss: 0.092\n",
            "==============================================\n",
            "epoch: 59\n",
            "step: 0, acc: 1.000, loss: 0.045 lr: 0.000982897582071948\n",
            "step: 1, acc: 1.000, loss: 0.034 lr: 0.0009828009828009828\n",
            "step: 2, acc: 1.000, loss: 0.009 lr: 0.0009827044025157233\n",
            "training, acc: 1.000, loss: 0.031 lr: 0.0009827044025157233\n",
            "validation, acc: 0.933, loss: 0.092\n",
            "==============================================\n",
            "epoch: 60\n",
            "step: 0, acc: 1.000, loss: 0.044 lr: 0.0009826078412105727\n",
            "step: 1, acc: 1.000, loss: 0.033 lr: 0.000982511298879937\n",
            "step: 2, acc: 1.000, loss: 0.009 lr: 0.0009824147755182239\n",
            "training, acc: 1.000, loss: 0.030 lr: 0.0009824147755182239\n",
            "validation, acc: 0.933, loss: 0.091\n",
            "==============================================\n",
            "epoch: 61\n",
            "step: 0, acc: 1.000, loss: 0.043 lr: 0.0009823182711198428\n",
            "step: 1, acc: 1.000, loss: 0.032 lr: 0.0009822217856792063\n",
            "step: 2, acc: 1.000, loss: 0.008 lr: 0.0009821253191907287\n",
            "training, acc: 1.000, loss: 0.029 lr: 0.0009821253191907287\n",
            "validation, acc: 0.933, loss: 0.090\n",
            "==============================================\n",
            "epoch: 62\n",
            "step: 0, acc: 1.000, loss: 0.043 lr: 0.0009820288716488265\n",
            "step: 1, acc: 1.000, loss: 0.030 lr: 0.0009819324430479185\n",
            "step: 2, acc: 1.000, loss: 0.008 lr: 0.0009818360333824251\n",
            "training, acc: 1.000, loss: 0.028 lr: 0.0009818360333824251\n",
            "validation, acc: 0.933, loss: 0.089\n",
            "==============================================\n",
            "epoch: 63\n",
            "step: 0, acc: 1.000, loss: 0.042 lr: 0.0009817396426467701\n",
            "step: 1, acc: 1.000, loss: 0.029 lr: 0.0009816432708353786\n",
            "step: 2, acc: 1.000, loss: 0.008 lr: 0.0009815469179426776\n",
            "training, acc: 1.000, loss: 0.028 lr: 0.0009815469179426776\n",
            "validation, acc: 0.933, loss: 0.089\n",
            "==============================================\n",
            "epoch: 64\n",
            "step: 0, acc: 1.000, loss: 0.041 lr: 0.0009814505839630975\n",
            "step: 1, acc: 1.000, loss: 0.028 lr: 0.0009813542688910698\n",
            "step: 2, acc: 1.000, loss: 0.007 lr: 0.0009812579727210284\n",
            "training, acc: 1.000, loss: 0.027 lr: 0.0009812579727210284\n",
            "validation, acc: 0.933, loss: 0.088\n",
            "==============================================\n",
            "epoch: 65\n",
            "step: 0, acc: 1.000, loss: 0.041 lr: 0.0009811616954474097\n",
            "step: 1, acc: 1.000, loss: 0.027 lr: 0.0009810654370646522\n",
            "step: 2, acc: 1.000, loss: 0.007 lr: 0.0009809691975671965\n",
            "training, acc: 1.000, loss: 0.026 lr: 0.0009809691975671965\n",
            "validation, acc: 0.933, loss: 0.087\n",
            "==============================================\n",
            "epoch: 66\n",
            "step: 0, acc: 1.000, loss: 0.040 lr: 0.0009808729769494849\n",
            "step: 1, acc: 1.000, loss: 0.027 lr: 0.000980776775205963\n",
            "step: 2, acc: 1.000, loss: 0.007 lr: 0.0009806805923310777\n",
            "training, acc: 1.000, loss: 0.026 lr: 0.0009806805923310777\n",
            "validation, acc: 0.933, loss: 0.087\n",
            "==============================================\n",
            "epoch: 67\n",
            "step: 0, acc: 1.000, loss: 0.040 lr: 0.0009805844283192783\n",
            "step: 1, acc: 1.000, loss: 0.026 lr: 0.0009804882831650162\n",
            "step: 2, acc: 1.000, loss: 0.006 lr: 0.000980392156862745\n",
            "training, acc: 1.000, loss: 0.025 lr: 0.000980392156862745\n",
            "validation, acc: 0.933, loss: 0.087\n",
            "==============================================\n",
            "epoch: 68\n",
            "step: 0, acc: 1.000, loss: 0.039 lr: 0.000980296049406921\n",
            "step: 1, acc: 1.000, loss: 0.025 lr: 0.0009801999607920015\n",
            "step: 2, acc: 1.000, loss: 0.006 lr: 0.0009801038910124474\n",
            "training, acc: 1.000, loss: 0.025 lr: 0.0009801038910124474\n",
            "validation, acc: 0.933, loss: 0.086\n",
            "==============================================\n",
            "epoch: 69\n",
            "step: 0, acc: 1.000, loss: 0.038 lr: 0.0009800078400627205\n",
            "step: 1, acc: 1.000, loss: 0.024 lr: 0.0009799118079372858\n",
            "step: 2, acc: 1.000, loss: 0.006 lr: 0.0009798157946306096\n",
            "training, acc: 1.000, loss: 0.024 lr: 0.0009798157946306096\n",
            "validation, acc: 0.933, loss: 0.086\n",
            "==============================================\n",
            "epoch: 70\n",
            "step: 0, acc: 1.000, loss: 0.038 lr: 0.000979719800137161\n",
            "step: 1, acc: 1.000, loss: 0.023 lr: 0.0009796238244514108\n",
            "step: 2, acc: 1.000, loss: 0.006 lr: 0.0009795278675678325\n",
            "training, acc: 1.000, loss: 0.023 lr: 0.0009795278675678325\n",
            "validation, acc: 0.933, loss: 0.085\n",
            "==============================================\n",
            "epoch: 71\n",
            "step: 0, acc: 1.000, loss: 0.037 lr: 0.0009794319294809011\n",
            "step: 1, acc: 1.000, loss: 0.023 lr: 0.0009793360101850947\n",
            "step: 2, acc: 1.000, loss: 0.006 lr: 0.0009792401096748921\n",
            "training, acc: 1.000, loss: 0.023 lr: 0.0009792401096748921\n",
            "validation, acc: 0.933, loss: 0.085\n",
            "==============================================\n",
            "epoch: 72\n",
            "step: 0, acc: 1.000, loss: 0.037 lr: 0.0009791442279447763\n",
            "step: 1, acc: 1.000, loss: 0.022 lr: 0.0009790483649892304\n",
            "step: 2, acc: 1.000, loss: 0.005 lr: 0.000978952520802741\n",
            "training, acc: 1.000, loss: 0.022 lr: 0.000978952520802741\n",
            "validation, acc: 0.933, loss: 0.085\n",
            "==============================================\n",
            "epoch: 73\n",
            "step: 0, acc: 1.000, loss: 0.036 lr: 0.0009788566953797965\n",
            "step: 1, acc: 1.000, loss: 0.021 lr: 0.0009787608887148868\n",
            "step: 2, acc: 1.000, loss: 0.005 lr: 0.0009786651008025053\n",
            "training, acc: 1.000, loss: 0.022 lr: 0.0009786651008025053\n",
            "validation, acc: 0.933, loss: 0.084\n",
            "==============================================\n",
            "epoch: 74\n",
            "step: 0, acc: 1.000, loss: 0.036 lr: 0.0009785693316371464\n",
            "step: 1, acc: 1.000, loss: 0.021 lr: 0.0009784735812133072\n",
            "step: 2, acc: 1.000, loss: 0.005 lr: 0.0009783778495254867\n",
            "training, acc: 1.000, loss: 0.022 lr: 0.0009783778495254867\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 75\n",
            "step: 0, acc: 1.000, loss: 0.035 lr: 0.0009782821365681863\n",
            "step: 1, acc: 1.000, loss: 0.020 lr: 0.0009781864423359092\n",
            "step: 2, acc: 1.000, loss: 0.005 lr: 0.0009780907668231612\n",
            "training, acc: 1.000, loss: 0.021 lr: 0.0009780907668231612\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 76\n",
            "step: 0, acc: 1.000, loss: 0.035 lr: 0.00097799511002445\n",
            "step: 1, acc: 1.000, loss: 0.020 lr: 0.0009778994719342852\n",
            "step: 2, acc: 1.000, loss: 0.005 lr: 0.000977803852547179\n",
            "training, acc: 1.000, loss: 0.021 lr: 0.000977803852547179\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 77\n",
            "step: 0, acc: 1.000, loss: 0.034 lr: 0.0009777082518576457\n",
            "step: 1, acc: 1.000, loss: 0.019 lr: 0.0009776126698602015\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009775171065493648\n",
            "training, acc: 1.000, loss: 0.020 lr: 0.0009775171065493648\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 78\n",
            "step: 0, acc: 1.000, loss: 0.034 lr: 0.000977421561919656\n",
            "step: 1, acc: 1.000, loss: 0.019 lr: 0.000977326035965598\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.000977230528681716\n",
            "training, acc: 1.000, loss: 0.020 lr: 0.000977230528681716\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 79\n",
            "step: 0, acc: 1.000, loss: 0.033 lr: 0.0009771350400625367\n",
            "step: 1, acc: 1.000, loss: 0.018 lr: 0.0009770395701025891\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009769441187964048\n",
            "training, acc: 1.000, loss: 0.019 lr: 0.0009769441187964048\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 80\n",
            "step: 0, acc: 1.000, loss: 0.033 lr: 0.000976848686138517\n",
            "step: 1, acc: 1.000, loss: 0.018 lr: 0.0009767532721234615\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.000976657876745776\n",
            "training, acc: 1.000, loss: 0.019 lr: 0.000976657876745776\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 81\n",
            "step: 0, acc: 1.000, loss: 0.032 lr: 0.0009765625\n",
            "step: 1, acc: 1.000, loss: 0.017 lr: 0.0009764671418806758\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009763718023823473\n",
            "training, acc: 1.000, loss: 0.019 lr: 0.0009763718023823473\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 82\n",
            "step: 0, acc: 1.000, loss: 0.032 lr: 0.0009762764814995607\n",
            "step: 1, acc: 1.000, loss: 0.017 lr: 0.0009761811792268646\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009760858955588092\n",
            "training, acc: 1.000, loss: 0.018 lr: 0.0009760858955588092\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 83\n",
            "step: 0, acc: 1.000, loss: 0.031 lr: 0.0009759906304899474\n",
            "step: 1, acc: 1.000, loss: 0.016 lr: 0.0009758953840148337\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009758001561280251\n",
            "training, acc: 1.000, loss: 0.018 lr: 0.0009758001561280251\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 84\n",
            "step: 0, acc: 1.000, loss: 0.031 lr: 0.0009757049468240805\n",
            "step: 1, acc: 1.000, loss: 0.016 lr: 0.0009756097560975611\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.00097551458394303\n",
            "training, acc: 1.000, loss: 0.018 lr: 0.00097551458394303\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 85\n",
            "step: 0, acc: 1.000, loss: 0.030 lr: 0.0009754194303550527\n",
            "step: 1, acc: 1.000, loss: 0.015 lr: 0.0009753242953281965\n",
            "step: 2, acc: 1.000, loss: 0.004 lr: 0.0009752291788570314\n",
            "training, acc: 1.000, loss: 0.017 lr: 0.0009752291788570314\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 86\n",
            "step: 0, acc: 1.000, loss: 0.030 lr: 0.0009751340809361287\n",
            "step: 1, acc: 1.000, loss: 0.015 lr: 0.0009750390015600623\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009749439407234084\n",
            "training, acc: 1.000, loss: 0.017 lr: 0.0009749439407234084\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 87\n",
            "step: 0, acc: 1.000, loss: 0.030 lr: 0.0009748488984207448\n",
            "step: 1, acc: 1.000, loss: 0.015 lr: 0.0009747538746466516\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009746588693957114\n",
            "training, acc: 1.000, loss: 0.017 lr: 0.0009746588693957114\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 88\n",
            "step: 0, acc: 1.000, loss: 0.029 lr: 0.0009745638826625085\n",
            "step: 1, acc: 1.000, loss: 0.014 lr: 0.0009744689144416293\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009743739647276625\n",
            "training, acc: 1.000, loss: 0.016 lr: 0.0009743739647276625\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 89\n",
            "step: 0, acc: 1.000, loss: 0.029 lr: 0.0009742790335151987\n",
            "step: 1, acc: 1.000, loss: 0.014 lr: 0.000974184120798831\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009740892265731541\n",
            "training, acc: 1.000, loss: 0.016 lr: 0.0009740892265731541\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 90\n",
            "step: 0, acc: 1.000, loss: 0.028 lr: 0.0009739943508327652\n",
            "step: 1, acc: 1.000, loss: 0.014 lr: 0.0009738994935722633\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009738046547862501\n",
            "training, acc: 1.000, loss: 0.016 lr: 0.0009738046547862501\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 91\n",
            "step: 0, acc: 1.000, loss: 0.028 lr: 0.0009737098344693283\n",
            "step: 1, acc: 1.000, loss: 0.013 lr: 0.0009736150326161038\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009735202492211838\n",
            "training, acc: 1.000, loss: 0.016 lr: 0.0009735202492211838\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 92\n",
            "step: 0, acc: 1.000, loss: 0.027 lr: 0.0009734254842791783\n",
            "step: 1, acc: 1.000, loss: 0.013 lr: 0.0009733307377846991\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.00097323600973236\n",
            "training, acc: 1.000, loss: 0.015 lr: 0.00097323600973236\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 93\n",
            "step: 0, acc: 1.000, loss: 0.027 lr: 0.0009731413001167769\n",
            "step: 1, acc: 1.000, loss: 0.013 lr: 0.0009730466089325678\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.000972951936174353\n",
            "training, acc: 1.000, loss: 0.015 lr: 0.000972951936174353\n",
            "validation, acc: 0.967, loss: 0.082\n",
            "==============================================\n",
            "epoch: 94\n",
            "step: 0, acc: 1.000, loss: 0.027 lr: 0.0009728572818367545\n",
            "step: 1, acc: 1.000, loss: 0.012 lr: 0.0009727626459143969\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009726680284019065\n",
            "training, acc: 1.000, loss: 0.015 lr: 0.0009726680284019065\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 95\n",
            "step: 0, acc: 1.000, loss: 0.026 lr: 0.0009725734292939116\n",
            "step: 1, acc: 1.000, loss: 0.012 lr: 0.0009724788485850433\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009723842862699339\n",
            "training, acc: 1.000, loss: 0.014 lr: 0.0009723842862699339\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 96\n",
            "step: 0, acc: 1.000, loss: 0.026 lr: 0.0009722897423432184\n",
            "step: 1, acc: 1.000, loss: 0.012 lr: 0.0009721952167995334\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009721007096335181\n",
            "training, acc: 1.000, loss: 0.014 lr: 0.0009721007096335181\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 97\n",
            "step: 0, acc: 1.000, loss: 0.026 lr: 0.0009720062208398135\n",
            "step: 1, acc: 1.000, loss: 0.012 lr: 0.0009719117504130625\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.0009718172983479106\n",
            "training, acc: 1.000, loss: 0.014 lr: 0.0009718172983479106\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 98\n",
            "step: 0, acc: 1.000, loss: 0.025 lr: 0.0009717228646390051\n",
            "step: 1, acc: 1.000, loss: 0.011 lr: 0.000971628449280995\n",
            "step: 2, acc: 1.000, loss: 0.003 lr: 0.000971534052268532\n",
            "training, acc: 1.000, loss: 0.014 lr: 0.000971534052268532\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 99\n",
            "step: 0, acc: 1.000, loss: 0.025 lr: 0.0009714396735962696\n",
            "step: 1, acc: 1.000, loss: 0.011 lr: 0.0009713453132588635\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009712509712509711\n",
            "training, acc: 1.000, loss: 0.014 lr: 0.0009712509712509711\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 100\n",
            "step: 0, acc: 1.000, loss: 0.024 lr: 0.0009711566475672526\n",
            "step: 1, acc: 1.000, loss: 0.011 lr: 0.0009710623422023694\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009709680551509856\n",
            "training, acc: 1.000, loss: 0.013 lr: 0.0009709680551509856\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 101\n",
            "step: 0, acc: 1.000, loss: 0.024 lr: 0.0009708737864077671\n",
            "step: 1, acc: 1.000, loss: 0.011 lr: 0.0009707795359673818\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009706853038245002\n",
            "training, acc: 1.000, loss: 0.013 lr: 0.0009706853038245002\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 102\n",
            "step: 0, acc: 1.000, loss: 0.024 lr: 0.0009705910899737941\n",
            "step: 1, acc: 1.000, loss: 0.010 lr: 0.000970496894409938\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009704027171276079\n",
            "training, acc: 1.000, loss: 0.013 lr: 0.0009704027171276079\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 103\n",
            "step: 0, acc: 1.000, loss: 0.023 lr: 0.0009703085581214826\n",
            "step: 1, acc: 1.000, loss: 0.010 lr: 0.0009702144173862424\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009701202949165697\n",
            "training, acc: 1.000, loss: 0.013 lr: 0.0009701202949165697\n",
            "validation, acc: 0.967, loss: 0.083\n",
            "==============================================\n",
            "epoch: 104\n",
            "step: 0, acc: 1.000, loss: 0.023 lr: 0.0009700261907071491\n",
            "step: 1, acc: 1.000, loss: 0.010 lr: 0.0009699321047526674\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009698380370478132\n",
            "training, acc: 1.000, loss: 0.012 lr: 0.0009698380370478132\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 105\n",
            "step: 0, acc: 1.000, loss: 0.023 lr: 0.0009697439875872772\n",
            "step: 1, acc: 1.000, loss: 0.010 lr: 0.0009696499563657519\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009695559433779328\n",
            "training, acc: 1.000, loss: 0.012 lr: 0.0009695559433779328\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 106\n",
            "step: 0, acc: 1.000, loss: 0.022 lr: 0.0009694619486185167\n",
            "step: 1, acc: 1.000, loss: 0.010 lr: 0.0009693679720822024\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009692740137636909\n",
            "training, acc: 1.000, loss: 0.012 lr: 0.0009692740137636909\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 107\n",
            "step: 0, acc: 1.000, loss: 0.022 lr: 0.0009691800736576856\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.0009690861517588913\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009689922480620155\n",
            "training, acc: 1.000, loss: 0.012 lr: 0.0009689922480620155\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 108\n",
            "step: 0, acc: 1.000, loss: 0.022 lr: 0.0009688983625617672\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.000968804495252858\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009687106461300011\n",
            "training, acc: 1.000, loss: 0.012 lr: 0.0009687106461300011\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 109\n",
            "step: 0, acc: 1.000, loss: 0.022 lr: 0.0009686168151879117\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.0009685230024213075\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.000968429207824908\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.000968429207824908\n",
            "validation, acc: 0.967, loss: 0.084\n",
            "==============================================\n",
            "epoch: 110\n",
            "step: 0, acc: 1.000, loss: 0.021 lr: 0.0009683354313934347\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.0009682416731216113\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009681479330041631\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.0009681479330041631\n",
            "validation, acc: 0.967, loss: 0.085\n",
            "==============================================\n",
            "epoch: 111\n",
            "step: 0, acc: 1.000, loss: 0.021 lr: 0.0009680542110358181\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.0009679605072113059\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009678668215253582\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.0009678668215253582\n",
            "validation, acc: 0.967, loss: 0.085\n",
            "==============================================\n",
            "epoch: 112\n",
            "step: 0, acc: 1.000, loss: 0.021 lr: 0.0009677731539727087\n",
            "step: 1, acc: 1.000, loss: 0.009 lr: 0.0009676795045480936\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009675858732462506\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.0009675858732462506\n",
            "validation, acc: 0.967, loss: 0.085\n",
            "==============================================\n",
            "epoch: 113\n",
            "step: 0, acc: 1.000, loss: 0.020 lr: 0.0009674922600619195\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009673986649898423\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.000967305088024763\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.000967305088024763\n",
            "validation, acc: 0.967, loss: 0.085\n",
            "==============================================\n",
            "epoch: 114\n",
            "step: 0, acc: 1.000, loss: 0.020 lr: 0.0009672115291614276\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009671179883945841\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009670244657189827\n",
            "training, acc: 1.000, loss: 0.011 lr: 0.0009670244657189827\n",
            "validation, acc: 0.967, loss: 0.086\n",
            "==============================================\n",
            "epoch: 115\n",
            "step: 0, acc: 1.000, loss: 0.020 lr: 0.0009669309611293754\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009668374746205163\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009667440061871616\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009667440061871616\n",
            "validation, acc: 0.967, loss: 0.086\n",
            "==============================================\n",
            "epoch: 116\n",
            "step: 0, acc: 1.000, loss: 0.020 lr: 0.0009666505558240697\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009665571235260005\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009664637092877163\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009664637092877163\n",
            "validation, acc: 0.967, loss: 0.086\n",
            "==============================================\n",
            "epoch: 117\n",
            "step: 0, acc: 1.000, loss: 0.019 lr: 0.0009663703131039815\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009662769349695623\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009661835748792271\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009661835748792271\n",
            "validation, acc: 0.967, loss: 0.086\n",
            "==============================================\n",
            "epoch: 118\n",
            "step: 0, acc: 1.000, loss: 0.019 lr: 0.0009660902328277462\n",
            "step: 1, acc: 1.000, loss: 0.008 lr: 0.0009659969088098919\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009659036028204385\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009659036028204385\n",
            "validation, acc: 0.967, loss: 0.086\n",
            "==============================================\n",
            "epoch: 119\n",
            "step: 0, acc: 1.000, loss: 0.019 lr: 0.0009658103148541626\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009657170449058426\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009656237929702587\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009656237929702587\n",
            "validation, acc: 0.967, loss: 0.087\n",
            "==============================================\n",
            "epoch: 120\n",
            "step: 0, acc: 1.000, loss: 0.018 lr: 0.0009655305590421936\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009654373431164317\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009653441451877594\n",
            "training, acc: 1.000, loss: 0.010 lr: 0.0009653441451877594\n",
            "validation, acc: 0.967, loss: 0.087\n",
            "==============================================\n",
            "epoch: 121\n",
            "step: 0, acc: 1.000, loss: 0.018 lr: 0.0009652509652509653\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009651578033008397\n",
            "step: 2, acc: 1.000, loss: 0.002 lr: 0.0009650646593321752\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009650646593321752\n",
            "validation, acc: 0.967, loss: 0.087\n",
            "==============================================\n",
            "epoch: 122\n",
            "step: 0, acc: 1.000, loss: 0.018 lr: 0.0009649715333397665\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009648784253184099\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.000964785335262904\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.000964785335262904\n",
            "validation, acc: 0.967, loss: 0.088\n",
            "==============================================\n",
            "epoch: 123\n",
            "step: 0, acc: 1.000, loss: 0.018 lr: 0.0009646922631680495\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009645992090286486\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009645061728395062\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009645061728395062\n",
            "validation, acc: 0.967, loss: 0.088\n",
            "==============================================\n",
            "epoch: 124\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.0009644131545954288\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009643201542912247\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009642271719217049\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009642271719217049\n",
            "validation, acc: 0.967, loss: 0.088\n",
            "==============================================\n",
            "epoch: 125\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.0009641342074816815\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009640412609659692\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009639483323693849\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009639483323693849\n",
            "validation, acc: 0.967, loss: 0.088\n",
            "==============================================\n",
            "epoch: 126\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.0009638554216867469\n",
            "step: 1, acc: 1.000, loss: 0.007 lr: 0.0009637625289128759\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009636696540425941\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009636696540425941\n",
            "validation, acc: 0.967, loss: 0.089\n",
            "==============================================\n",
            "epoch: 127\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.0009635767970707265\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009634839579920994\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009633911368015414\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009633911368015414\n",
            "validation, acc: 0.967, loss: 0.089\n",
            "==============================================\n",
            "epoch: 128\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.000963298333493883\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009632055480639569\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009631127805065973\n",
            "training, acc: 1.000, loss: 0.009 lr: 0.0009631127805065973\n",
            "validation, acc: 0.967, loss: 0.089\n",
            "==============================================\n",
            "epoch: 129\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.000963020030816641\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009629272989889264\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009628345850182939\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009628345850182939\n",
            "validation, acc: 0.967, loss: 0.090\n",
            "==============================================\n",
            "epoch: 130\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.000962741888899586\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009626492106276474\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009625565501973242\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009625565501973242\n",
            "validation, acc: 0.967, loss: 0.090\n",
            "==============================================\n",
            "epoch: 131\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.0009624639076034649\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009623712828409201\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009622786759045421\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009622786759045421\n",
            "validation, acc: 0.967, loss: 0.090\n",
            "==============================================\n",
            "epoch: 132\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.0009621860867891852\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009620935154897055\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.000962000962000962\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.000962000962000962\n",
            "validation, acc: 0.967, loss: 0.090\n",
            "==============================================\n",
            "epoch: 133\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.0009619084263178145\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009618159084351255\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009617234083477592\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009617234083477592\n",
            "validation, acc: 0.967, loss: 0.091\n",
            "==============================================\n",
            "epoch: 134\n",
            "step: 0, acc: 1.000, loss: 0.015 lr: 0.0009616309260505818\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009615384615384615\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009614460148062685\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009614460148062685\n",
            "validation, acc: 0.967, loss: 0.091\n",
            "==============================================\n",
            "epoch: 135\n",
            "step: 0, acc: 1.000, loss: 0.015 lr: 0.0009613535858488752\n",
            "step: 1, acc: 1.000, loss: 0.006 lr: 0.0009612611746611555\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009611687812379854\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009611687812379854\n",
            "validation, acc: 0.967, loss: 0.091\n",
            "==============================================\n",
            "epoch: 136\n",
            "step: 0, acc: 1.000, loss: 0.015 lr: 0.0009610764055742432\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009609840476648089\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009608917075045643\n",
            "training, acc: 1.000, loss: 0.008 lr: 0.0009608917075045643\n",
            "validation, acc: 0.967, loss: 0.092\n",
            "==============================================\n",
            "epoch: 137\n",
            "step: 0, acc: 1.000, loss: 0.015 lr: 0.0009607993850883936\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009607070804111827\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009606147934678196\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009606147934678196\n",
            "validation, acc: 0.967, loss: 0.092\n",
            "==============================================\n",
            "epoch: 138\n",
            "step: 0, acc: 1.000, loss: 0.015 lr: 0.0009605225242531938\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009604302727621976\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009603380389897242\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009603380389897242\n",
            "validation, acc: 0.967, loss: 0.092\n",
            "==============================================\n",
            "epoch: 139\n",
            "step: 0, acc: 1.000, loss: 0.014 lr: 0.0009602458229306702\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009601536245799327\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009600614439324116\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009600614439324116\n",
            "validation, acc: 0.967, loss: 0.093\n",
            "==============================================\n",
            "epoch: 140\n",
            "step: 0, acc: 1.000, loss: 0.014 lr: 0.0009599692809830085\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009598771357266269\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009597850081581725\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009597850081581725\n",
            "validation, acc: 0.967, loss: 0.093\n",
            "==============================================\n",
            "epoch: 141\n",
            "step: 0, acc: 1.000, loss: 0.014 lr: 0.0009596928982725527\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009596008060646772\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.000959508731529457\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.000959508731529457\n",
            "validation, acc: 0.967, loss: 0.093\n",
            "==============================================\n",
            "epoch: 142\n",
            "step: 0, acc: 1.000, loss: 0.014 lr: 0.0009594166746618056\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009593246354566385\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009592326139088729\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009592326139088729\n",
            "validation, acc: 0.967, loss: 0.094\n",
            "==============================================\n",
            "epoch: 143\n",
            "step: 0, acc: 1.000, loss: 0.014 lr: 0.000959140610013428\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009590486237652249\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009589566551591868\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009589566551591868\n",
            "validation, acc: 0.967, loss: 0.094\n",
            "==============================================\n",
            "epoch: 144\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009588647041902388\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009587727708533078\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.000958680855143323\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.000958680855143323\n",
            "validation, acc: 0.967, loss: 0.094\n",
            "==============================================\n",
            "epoch: 145\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009585889570552148\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009584970765839165\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009584052137243626\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009584052137243626\n",
            "validation, acc: 0.967, loss: 0.094\n",
            "==============================================\n",
            "epoch: 146\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009583133684714901\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009582215408202376\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009581297307655457\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009581297307655457\n",
            "validation, acc: 0.967, loss: 0.095\n",
            "==============================================\n",
            "epoch: 147\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009580379383023567\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.0009579461634256155\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009578544061302681\n",
            "training, acc: 1.000, loss: 0.007 lr: 0.0009578544061302681\n",
            "validation, acc: 0.967, loss: 0.095\n",
            "==============================================\n",
            "epoch: 148\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009577626664112633\n",
            "step: 1, acc: 1.000, loss: 0.005 lr: 0.000957670944263551\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009575792396820837\n",
            "training, acc: 1.000, loss: 0.006 lr: 0.0009575792396820837\n",
            "validation, acc: 0.967, loss: 0.095\n",
            "==============================================\n",
            "epoch: 149\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0009574875526618154\n",
            "step: 1, acc: 1.000, loss: 0.004 lr: 0.0009573958831977024\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009573042312847023\n",
            "training, acc: 1.000, loss: 0.006 lr: 0.0009573042312847023\n",
            "validation, acc: 0.967, loss: 0.096\n",
            "==============================================\n",
            "epoch: 150\n",
            "step: 0, acc: 1.000, loss: 0.012 lr: 0.0009572125969177756\n",
            "step: 1, acc: 1.000, loss: 0.004 lr: 0.0009571209800918836\n",
            "step: 2, acc: 1.000, loss: 0.001 lr: 0.0009570293808019906\n",
            "training, acc: 1.000, loss: 0.006 lr: 0.0009570293808019906\n",
            "validation, acc: 0.967, loss: 0.096\n",
            "Evaluation on test set\n",
            "validation, acc: 0.967, loss: 0.095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IjTa6tU34ob"
      },
      "source": [
        "## Training and Evaluation: MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RO3fz1-0PkO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b8e7d1e-17d2-475f-9938-03e8db9d1be5"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data and split into 3 sets\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "X_train, X_validate, Y_train, Y_validate = train_test_split(\n",
        "    X_train, Y_train, test_size=0.2, random_state=789)\n",
        "\n",
        "# Preprocessing\n",
        "# Scale and reshape samples\n",
        "X_train = (X_train.reshape(X_train.shape[0], -1).astype(np.float32) - \n",
        "            127.5) / 127.5\n",
        "X_validate = (X_validate.reshape(X_validate.shape[0], -1).astype(np.float32) - \n",
        "            127.5) / 127.5   \n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "# Instantiate the model\n",
        "modelr = Model()\n",
        "\n",
        "# Add layers\n",
        "modelr.add(Layer_Dense(X_train.shape[1], 128))\n",
        "modelr.add(Activation_ReLU())\n",
        "modelr.add(Layer_Dense(128, 128))\n",
        "modelr.add(Activation_ReLU())\n",
        "modelr.add(Layer_Dense(128, 10))\n",
        "modelr.add(Activation_Softmax())\n",
        "\n",
        "# Set loss, optimizer and accuracy objects\n",
        "modelr.set(\n",
        "    loss=Loss_CategoricalCrossentropy(),\n",
        "    optimizer=Optimizer_Adam(decay=1e-4),\n",
        "    accuracy=Accuracy_Categorical()\n",
        ")\n",
        "\n",
        "# Finalize the model\n",
        "modelr.finalize()\n",
        "\n",
        "# Train the model\n",
        "modelr.train(X_train, Y_train, validation_data=(X_validate, Y_validate),\n",
        "            epochs=62, batch_size=128, print_every=200)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluation on test set\")\n",
        "modelr.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================================\n",
            "epoch: 1\n",
            "step: 0, acc: 0.078, loss: 2.302 lr: 0.001\n",
            "step: 200, acc: 0.891, loss: 0.379 lr: 0.000980392156862745\n",
            "step: 374, acc: 0.875, loss: 0.502 lr: 0.0009639483323693849\n",
            "training, acc: 0.789, loss: 0.670 lr: 0.0009639483323693849\n",
            "validation, acc: 0.895, loss: 0.358\n",
            "==============================================\n",
            "epoch: 2\n",
            "step: 0, acc: 0.914, loss: 0.360 lr: 0.0009638554216867469\n",
            "step: 200, acc: 0.914, loss: 0.251 lr: 0.0009456264775413711\n",
            "step: 374, acc: 0.906, loss: 0.383 lr: 0.0009303190994511118\n",
            "training, acc: 0.910, loss: 0.305 lr: 0.0009303190994511118\n",
            "validation, acc: 0.921, loss: 0.264\n",
            "==============================================\n",
            "epoch: 3\n",
            "step: 0, acc: 0.906, loss: 0.262 lr: 0.0009302325581395349\n",
            "step: 200, acc: 0.930, loss: 0.201 lr: 0.0009132420091324202\n",
            "step: 374, acc: 0.930, loss: 0.301 lr: 0.0008989572096368213\n",
            "training, acc: 0.933, loss: 0.228 lr: 0.0008989572096368213\n",
            "validation, acc: 0.936, loss: 0.215\n",
            "==============================================\n",
            "epoch: 4\n",
            "step: 0, acc: 0.938, loss: 0.216 lr: 0.0008988764044943821\n",
            "step: 200, acc: 0.922, loss: 0.152 lr: 0.0008830022075055188\n",
            "step: 374, acc: 0.938, loss: 0.270 lr: 0.0008696408383337683\n",
            "training, acc: 0.946, loss: 0.182 lr: 0.0008696408383337683\n",
            "validation, acc: 0.944, loss: 0.185\n",
            "==============================================\n",
            "epoch: 5\n",
            "step: 0, acc: 0.945, loss: 0.177 lr: 0.0008695652173913045\n",
            "step: 200, acc: 0.945, loss: 0.119 lr: 0.0008547008547008548\n",
            "step: 374, acc: 0.938, loss: 0.252 lr: 0.0008421761832575375\n",
            "training, acc: 0.955, loss: 0.151 lr: 0.0008421761832575375\n",
            "validation, acc: 0.950, loss: 0.164\n",
            "==============================================\n",
            "epoch: 6\n",
            "step: 0, acc: 0.969, loss: 0.116 lr: 0.0008421052631578947\n",
            "step: 200, acc: 0.969, loss: 0.103 lr: 0.0008281573498964803\n",
            "step: 374, acc: 0.930, loss: 0.229 lr: 0.0008163931749530574\n",
            "training, acc: 0.962, loss: 0.129 lr: 0.0008163931749530574\n",
            "validation, acc: 0.953, loss: 0.152\n",
            "==============================================\n",
            "epoch: 7\n",
            "step: 0, acc: 0.977, loss: 0.074 lr: 0.0008163265306122448\n",
            "step: 200, acc: 0.977, loss: 0.092 lr: 0.0008032128514056224\n",
            "step: 374, acc: 0.953, loss: 0.212 lr: 0.0007921419518377694\n",
            "training, acc: 0.966, loss: 0.112 lr: 0.0007921419518377694\n",
            "validation, acc: 0.955, loss: 0.144\n",
            "==============================================\n",
            "epoch: 8\n",
            "step: 0, acc: 0.977, loss: 0.058 lr: 0.0007920792079207921\n",
            "step: 200, acc: 0.977, loss: 0.080 lr: 0.0007797270955165693\n",
            "step: 374, acc: 0.961, loss: 0.197 lr: 0.0007692899453804139\n",
            "training, acc: 0.970, loss: 0.099 lr: 0.0007692899453804139\n",
            "validation, acc: 0.956, loss: 0.143\n",
            "==============================================\n",
            "epoch: 9\n",
            "step: 0, acc: 0.977, loss: 0.049 lr: 0.0007692307692307692\n",
            "step: 200, acc: 0.977, loss: 0.073 lr: 0.0007575757575757576\n",
            "step: 374, acc: 0.961, loss: 0.179 lr: 0.0007477194556602362\n",
            "training, acc: 0.973, loss: 0.088 lr: 0.0007477194556602362\n",
            "validation, acc: 0.958, loss: 0.140\n",
            "==============================================\n",
            "epoch: 10\n",
            "step: 0, acc: 1.000, loss: 0.036 lr: 0.0007476635514018692\n",
            "step: 200, acc: 0.977, loss: 0.065 lr: 0.0007366482504604051\n",
            "step: 374, acc: 0.961, loss: 0.169 lr: 0.0007273256236817223\n",
            "training, acc: 0.976, loss: 0.079 lr: 0.0007273256236817223\n",
            "validation, acc: 0.959, loss: 0.136\n",
            "==============================================\n",
            "epoch: 11\n",
            "step: 0, acc: 1.000, loss: 0.027 lr: 0.0007272727272727273\n",
            "step: 200, acc: 0.984, loss: 0.062 lr: 0.0007168458781362007\n",
            "step: 374, acc: 0.969, loss: 0.154 lr: 0.0007080147267063155\n",
            "training, acc: 0.979, loss: 0.071 lr: 0.0007080147267063155\n",
            "validation, acc: 0.960, loss: 0.136\n",
            "==============================================\n",
            "epoch: 12\n",
            "step: 0, acc: 1.000, loss: 0.025 lr: 0.0007079646017699115\n",
            "step: 200, acc: 0.984, loss: 0.059 lr: 0.0006980802792321117\n",
            "step: 374, acc: 0.969, loss: 0.145 lr: 0.0006897027381198704\n",
            "training, acc: 0.981, loss: 0.064 lr: 0.0006897027381198704\n",
            "validation, acc: 0.961, loss: 0.134\n",
            "==============================================\n",
            "epoch: 13\n",
            "step: 0, acc: 1.000, loss: 0.025 lr: 0.0006896551724137932\n",
            "step: 200, acc: 0.984, loss: 0.064 lr: 0.0006802721088435374\n",
            "step: 374, acc: 0.977, loss: 0.139 lr: 0.0006723141051499261\n",
            "training, acc: 0.983, loss: 0.059 lr: 0.0006723141051499261\n",
            "validation, acc: 0.962, loss: 0.131\n",
            "==============================================\n",
            "epoch: 14\n",
            "step: 0, acc: 1.000, loss: 0.021 lr: 0.0006722689075630251\n",
            "step: 200, acc: 0.984, loss: 0.062 lr: 0.0006633499170812604\n",
            "step: 374, acc: 0.977, loss: 0.120 lr: 0.000655780706931602\n",
            "training, acc: 0.984, loss: 0.053 lr: 0.000655780706931602\n",
            "validation, acc: 0.966, loss: 0.122\n",
            "==============================================\n",
            "epoch: 15\n",
            "step: 0, acc: 1.000, loss: 0.016 lr: 0.0006557377049180329\n",
            "step: 200, acc: 0.984, loss: 0.060 lr: 0.0006472491909385114\n",
            "step: 374, acc: 0.977, loss: 0.101 lr: 0.0006400409626216078\n",
            "training, acc: 0.985, loss: 0.049 lr: 0.0006400409626216078\n",
            "validation, acc: 0.965, loss: 0.122\n",
            "==============================================\n",
            "epoch: 16\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.00064\n",
            "step: 200, acc: 0.984, loss: 0.053 lr: 0.0006319115323854661\n",
            "step: 374, acc: 0.984, loss: 0.086 lr: 0.0006250390649415589\n",
            "training, acc: 0.987, loss: 0.044 lr: 0.0006250390649415589\n",
            "validation, acc: 0.965, loss: 0.122\n",
            "==============================================\n",
            "epoch: 17\n",
            "step: 0, acc: 1.000, loss: 0.017 lr: 0.000625\n",
            "step: 200, acc: 0.984, loss: 0.052 lr: 0.0006172839506172839\n",
            "step: 374, acc: 0.977, loss: 0.070 lr: 0.0006107243190423843\n",
            "training, acc: 0.989, loss: 0.040 lr: 0.0006107243190423843\n",
            "validation, acc: 0.967, loss: 0.114\n",
            "==============================================\n",
            "epoch: 18\n",
            "step: 0, acc: 1.000, loss: 0.013 lr: 0.0006106870229007633\n",
            "step: 200, acc: 0.984, loss: 0.044 lr: 0.0006033182503770739\n",
            "step: 374, acc: 0.984, loss: 0.055 lr: 0.0005970505701832946\n",
            "training, acc: 0.990, loss: 0.036 lr: 0.0005970505701832946\n",
            "validation, acc: 0.969, loss: 0.108\n",
            "==============================================\n",
            "epoch: 19\n",
            "step: 0, acc: 1.000, loss: 0.011 lr: 0.0005970149253731343\n",
            "step: 200, acc: 0.984, loss: 0.034 lr: 0.0005899705014749262\n",
            "step: 374, acc: 0.984, loss: 0.049 lr: 0.0005839757066106049\n",
            "training, acc: 0.991, loss: 0.033 lr: 0.0005839757066106049\n",
            "validation, acc: 0.969, loss: 0.114\n",
            "==============================================\n",
            "epoch: 20\n",
            "step: 0, acc: 0.992, loss: 0.014 lr: 0.0005839416058394161\n",
            "step: 200, acc: 0.984, loss: 0.028 lr: 0.0005772005772005773\n",
            "step: 374, acc: 0.992, loss: 0.040 lr: 0.0005714612263557917\n",
            "training, acc: 0.992, loss: 0.030 lr: 0.0005714612263557917\n",
            "validation, acc: 0.968, loss: 0.119\n",
            "==============================================\n",
            "epoch: 21\n",
            "step: 0, acc: 0.992, loss: 0.019 lr: 0.0005714285714285714\n",
            "step: 200, acc: 0.984, loss: 0.031 lr: 0.0005649717514124294\n",
            "step: 374, acc: 0.992, loss: 0.028 lr: 0.0005594718585655142\n",
            "training, acc: 0.992, loss: 0.028 lr: 0.0005594718585655142\n",
            "validation, acc: 0.969, loss: 0.120\n",
            "==============================================\n",
            "epoch: 22\n",
            "step: 0, acc: 0.992, loss: 0.017 lr: 0.0005594405594405593\n",
            "step: 200, acc: 0.977, loss: 0.042 lr: 0.0005532503457814661\n",
            "step: 374, acc: 0.992, loss: 0.029 lr: 0.0005479752315195353\n",
            "training, acc: 0.992, loss: 0.028 lr: 0.0005479752315195353\n",
            "validation, acc: 0.970, loss: 0.122\n",
            "==============================================\n",
            "epoch: 23\n",
            "step: 0, acc: 0.992, loss: 0.013 lr: 0.000547945205479452\n",
            "step: 200, acc: 0.977, loss: 0.031 lr: 0.0005420054200542005\n",
            "step: 374, acc: 1.000, loss: 0.022 lr: 0.0005369415807560138\n",
            "training, acc: 0.992, loss: 0.027 lr: 0.0005369415807560138\n",
            "validation, acc: 0.972, loss: 0.115\n",
            "==============================================\n",
            "epoch: 24\n",
            "step: 0, acc: 0.992, loss: 0.007 lr: 0.0005369127516778524\n",
            "step: 200, acc: 0.984, loss: 0.028 lr: 0.0005312084993359894\n",
            "step: 374, acc: 0.984, loss: 0.028 lr: 0.0005263434917627244\n",
            "training, acc: 0.993, loss: 0.024 lr: 0.0005263434917627244\n",
            "validation, acc: 0.972, loss: 0.116\n",
            "==============================================\n",
            "epoch: 25\n",
            "step: 0, acc: 1.000, loss: 0.006 lr: 0.0005263157894736842\n",
            "step: 200, acc: 0.992, loss: 0.025 lr: 0.0005208333333333334\n",
            "step: 374, acc: 0.992, loss: 0.027 lr: 0.0005161556725508414\n",
            "training, acc: 0.993, loss: 0.022 lr: 0.0005161556725508414\n",
            "validation, acc: 0.972, loss: 0.124\n",
            "==============================================\n",
            "epoch: 26\n",
            "step: 0, acc: 1.000, loss: 0.004 lr: 0.0005161290322580645\n",
            "step: 200, acc: 1.000, loss: 0.021 lr: 0.0005108556832694764\n",
            "step: 374, acc: 1.000, loss: 0.016 lr: 0.0005063547521393488\n",
            "training, acc: 0.994, loss: 0.020 lr: 0.0005063547521393488\n",
            "validation, acc: 0.972, loss: 0.127\n",
            "==============================================\n",
            "epoch: 27\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.0005063291139240506\n",
            "step: 200, acc: 1.000, loss: 0.006 lr: 0.0005012531328320802\n",
            "step: 374, acc: 0.992, loss: 0.016 lr: 0.0004969191015702644\n",
            "training, acc: 0.995, loss: 0.017 lr: 0.0004969191015702644\n",
            "validation, acc: 0.968, loss: 0.139\n",
            "==============================================\n",
            "epoch: 28\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.0004968944099378881\n",
            "step: 200, acc: 1.000, loss: 0.009 lr: 0.0004920049200492005\n",
            "step: 374, acc: 1.000, loss: 0.020 lr: 0.0004878286745694912\n",
            "training, acc: 0.995, loss: 0.017 lr: 0.0004878286745694912\n",
            "validation, acc: 0.972, loss: 0.127\n",
            "==============================================\n",
            "epoch: 29\n",
            "step: 0, acc: 1.000, loss: 0.004 lr: 0.00048780487804878054\n",
            "step: 200, acc: 1.000, loss: 0.007 lr: 0.00048309178743961346\n",
            "step: 374, acc: 0.984, loss: 0.037 lr: 0.00047906486538277284\n",
            "training, acc: 0.996, loss: 0.015 lr: 0.00047906486538277284\n",
            "validation, acc: 0.969, loss: 0.138\n",
            "==============================================\n",
            "epoch: 30\n",
            "step: 0, acc: 1.000, loss: 0.004 lr: 0.0004790419161676646\n",
            "step: 200, acc: 1.000, loss: 0.008 lr: 0.0004744958481613286\n",
            "step: 374, acc: 0.992, loss: 0.022 lr: 0.0004706103816650195\n",
            "training, acc: 0.996, loss: 0.014 lr: 0.0004706103816650195\n",
            "validation, acc: 0.971, loss: 0.132\n",
            "==============================================\n",
            "epoch: 31\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.00047058823529411766\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.0004662004662004662\n",
            "step: 374, acc: 0.992, loss: 0.019 lr: 0.00046244913059563454\n",
            "training, acc: 0.996, loss: 0.012 lr: 0.00046244913059563454\n",
            "validation, acc: 0.969, loss: 0.143\n",
            "==============================================\n",
            "epoch: 32\n",
            "step: 0, acc: 1.000, loss: 0.005 lr: 0.00046242774566473987\n",
            "step: 200, acc: 1.000, loss: 0.007 lr: 0.0004581901489117984\n",
            "step: 374, acc: 1.000, loss: 0.013 lr: 0.0004545661166416655\n",
            "training, acc: 0.997, loss: 0.012 lr: 0.0004545661166416655\n",
            "validation, acc: 0.968, loss: 0.153\n",
            "==============================================\n",
            "epoch: 33\n",
            "step: 0, acc: 1.000, loss: 0.007 lr: 0.00045454545454545455\n",
            "step: 200, acc: 1.000, loss: 0.006 lr: 0.0004504504504504505\n",
            "step: 374, acc: 1.000, loss: 0.008 lr: 0.00044694734960221686\n",
            "training, acc: 0.997, loss: 0.011 lr: 0.00044694734960221686\n",
            "validation, acc: 0.972, loss: 0.137\n",
            "==============================================\n",
            "epoch: 34\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.00044692737430167604\n",
            "step: 200, acc: 1.000, loss: 0.010 lr: 0.00044296788482834986\n",
            "step: 374, acc: 0.992, loss: 0.017 lr: 0.0004395797617477691\n",
            "training, acc: 0.997, loss: 0.010 lr: 0.0004395797617477691\n",
            "validation, acc: 0.969, loss: 0.145\n",
            "==============================================\n",
            "epoch: 35\n",
            "step: 0, acc: 1.000, loss: 0.002 lr: 0.0004395604395604395\n",
            "step: 200, acc: 1.000, loss: 0.005 lr: 0.0004357298474945534\n",
            "step: 374, acc: 0.992, loss: 0.016 lr: 0.0004324511330219685\n",
            "training, acc: 0.998, loss: 0.009 lr: 0.0004324511330219685\n",
            "validation, acc: 0.972, loss: 0.141\n",
            "==============================================\n",
            "epoch: 36\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.0004324324324324325\n",
            "step: 200, acc: 1.000, loss: 0.008 lr: 0.0004287245444801715\n",
            "step: 374, acc: 1.000, loss: 0.005 lr: 0.00042555002340525133\n",
            "training, acc: 0.997, loss: 0.009 lr: 0.00042555002340525133\n",
            "validation, acc: 0.974, loss: 0.134\n",
            "==============================================\n",
            "epoch: 37\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.000425531914893617\n",
            "step: 200, acc: 1.000, loss: 0.008 lr: 0.00042194092827004215\n",
            "step: 374, acc: 0.992, loss: 0.014 lr: 0.0004188657116528441\n",
            "training, acc: 0.998, loss: 0.009 lr: 0.0004188657116528441\n",
            "validation, acc: 0.974, loss: 0.129\n",
            "==============================================\n",
            "epoch: 38\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.00041884816753926695\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.00041536863966770514\n",
            "step: 374, acc: 1.000, loss: 0.006 lr: 0.0004123881397171018\n",
            "training, acc: 0.998, loss: 0.007 lr: 0.0004123881397171018\n",
            "validation, acc: 0.974, loss: 0.134\n",
            "==============================================\n",
            "epoch: 39\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.0004123711340206186\n",
            "step: 200, acc: 1.000, loss: 0.005 lr: 0.0004089979550102249\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.0004061078622482131\n",
            "training, acc: 0.998, loss: 0.008 lr: 0.0004061078622482131\n",
            "validation, acc: 0.973, loss: 0.136\n",
            "==============================================\n",
            "epoch: 40\n",
            "step: 0, acc: 1.000, loss: 0.002 lr: 0.00040609137055837557\n",
            "step: 200, acc: 1.000, loss: 0.005 lr: 0.0004028197381671702\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.0004000160006400256\n",
            "training, acc: 0.998, loss: 0.008 lr: 0.0004000160006400256\n",
            "validation, acc: 0.973, loss: 0.143\n",
            "==============================================\n",
            "epoch: 41\n",
            "step: 0, acc: 1.000, loss: 0.004 lr: 0.0004\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.0003968253968253968\n",
            "step: 374, acc: 0.992, loss: 0.019 lr: 0.0003941042011507843\n",
            "training, acc: 0.998, loss: 0.007 lr: 0.0003941042011507843\n",
            "validation, acc: 0.974, loss: 0.133\n",
            "==============================================\n",
            "epoch: 42\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.0003940886699507389\n",
            "step: 200, acc: 1.000, loss: 0.001 lr: 0.00039100684261974585\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.0003883645966833664\n",
            "training, acc: 0.998, loss: 0.006 lr: 0.0003883645966833664\n",
            "validation, acc: 0.974, loss: 0.141\n",
            "==============================================\n",
            "epoch: 43\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003883495145631068\n",
            "step: 200, acc: 1.000, loss: 0.003 lr: 0.0003853564547206166\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.000382789771857296\n",
            "training, acc: 0.999, loss: 0.006 lr: 0.000382789771857296\n",
            "validation, acc: 0.974, loss: 0.141\n",
            "==============================================\n",
            "epoch: 44\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003827751196172249\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.0003798670465337132\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.0003773727310464546\n",
            "training, acc: 0.999, loss: 0.005 lr: 0.0003773727310464546\n",
            "validation, acc: 0.974, loss: 0.142\n",
            "==============================================\n",
            "epoch: 45\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003773584905660377\n",
            "step: 200, acc: 1.000, loss: 0.001 lr: 0.0003745318352059925\n",
            "step: 374, acc: 0.992, loss: 0.012 lr: 0.00037210686909280345\n",
            "training, acc: 0.998, loss: 0.006 lr: 0.00037210686909280345\n",
            "validation, acc: 0.973, loss: 0.145\n",
            "==============================================\n",
            "epoch: 46\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.00037209302325581393\n",
            "step: 200, acc: 1.000, loss: 0.002 lr: 0.0003693444136657433\n",
            "step: 374, acc: 1.000, loss: 0.001 lr: 0.00036698594443832806\n",
            "training, acc: 0.999, loss: 0.005 lr: 0.00036698594443832806\n",
            "validation, acc: 0.973, loss: 0.146\n",
            "==============================================\n",
            "epoch: 47\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003669724770642202\n",
            "step: 200, acc: 1.000, loss: 0.003 lr: 0.00036429872495446266\n",
            "step: 374, acc: 0.992, loss: 0.008 lr: 0.00036200405444540974\n",
            "training, acc: 0.999, loss: 0.005 lr: 0.00036200405444540974\n",
            "validation, acc: 0.974, loss: 0.146\n",
            "==============================================\n",
            "epoch: 48\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.00036199095022624434\n",
            "step: 200, acc: 0.992, loss: 0.009 lr: 0.00035938903863432165\n",
            "step: 374, acc: 1.000, loss: 0.004 lr: 0.0003571556127004536\n",
            "training, acc: 0.999, loss: 0.004 lr: 0.0003571556127004536\n",
            "validation, acc: 0.974, loss: 0.147\n",
            "==============================================\n",
            "epoch: 49\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.00035714285714285714\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.00035460992907801415\n",
            "step: 374, acc: 1.000, loss: 0.001 lr: 0.00035243532811729045\n",
            "training, acc: 0.999, loss: 0.004 lr: 0.00035243532811729045\n",
            "validation, acc: 0.974, loss: 0.146\n",
            "==============================================\n",
            "epoch: 50\n",
            "step: 0, acc: 1.000, loss: 0.002 lr: 0.00035242290748898676\n",
            "step: 200, acc: 1.000, loss: 0.003 lr: 0.0003499562554680665\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.0003478381856760235\n",
            "training, acc: 0.999, loss: 0.004 lr: 0.0003478381856760235\n",
            "validation, acc: 0.974, loss: 0.146\n",
            "==============================================\n",
            "epoch: 51\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.00034782608695652176\n",
            "step: 200, acc: 1.000, loss: 0.003 lr: 0.0003454231433506045\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.00034335942864991076\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.00034335942864991076\n",
            "validation, acc: 0.973, loss: 0.156\n",
            "==============================================\n",
            "epoch: 52\n",
            "step: 0, acc: 1.000, loss: 0.003 lr: 0.00034334763948497857\n",
            "step: 200, acc: 1.000, loss: 0.002 lr: 0.00034100596760443307\n",
            "step: 374, acc: 0.992, loss: 0.018 lr: 0.00033899454218787073\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.00033899454218787073\n",
            "validation, acc: 0.972, loss: 0.155\n",
            "==============================================\n",
            "epoch: 53\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003389830508474576\n",
            "step: 200, acc: 1.000, loss: 0.005 lr: 0.00033670033670033666\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.000334739238133494\n",
            "training, acc: 1.000, loss: 0.003 lr: 0.000334739238133494\n",
            "validation, acc: 0.973, loss: 0.161\n",
            "==============================================\n",
            "epoch: 54\n",
            "step: 0, acc: 1.000, loss: 0.004 lr: 0.00033472803347280337\n",
            "step: 200, acc: 1.000, loss: 0.002 lr: 0.0003325020781379883\n",
            "step: 374, acc: 1.000, loss: 0.005 lr: 0.0003305894409732553\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.0003305894409732553\n",
            "validation, acc: 0.973, loss: 0.158\n",
            "==============================================\n",
            "epoch: 55\n",
            "step: 0, acc: 1.000, loss: 0.002 lr: 0.00033057851239669424\n",
            "step: 200, acc: 1.000, loss: 0.001 lr: 0.0003284072249589491\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.00032654127481713684\n",
            "training, acc: 1.000, loss: 0.002 lr: 0.00032654127481713684\n",
            "validation, acc: 0.973, loss: 0.158\n",
            "==============================================\n",
            "epoch: 56\n",
            "step: 0, acc: 1.000, loss: 0.002 lr: 0.00032653061224489796\n",
            "step: 200, acc: 1.000, loss: 0.003 lr: 0.00032441200324412003\n",
            "step: 374, acc: 1.000, loss: 0.007 lr: 0.0003225910513242362\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.0003225910513242362\n",
            "validation, acc: 0.974, loss: 0.162\n",
            "==============================================\n",
            "epoch: 57\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003225806451612903\n",
            "step: 200, acc: 0.992, loss: 0.007 lr: 0.0003205128205128205\n",
            "step: 374, acc: 1.000, loss: 0.000 lr: 0.00031873525849429466\n",
            "training, acc: 1.000, loss: 0.002 lr: 0.00031873525849429466\n",
            "validation, acc: 0.974, loss: 0.155\n",
            "==============================================\n",
            "epoch: 58\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.00031872509960159364\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.0003167062549485352\n",
            "step: 374, acc: 1.000, loss: 0.003 lr: 0.0003149705502535513\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.0003149705502535513\n",
            "validation, acc: 0.973, loss: 0.160\n",
            "==============================================\n",
            "epoch: 59\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.00031496062992125983\n",
            "step: 200, acc: 1.000, loss: 0.004 lr: 0.00031298904538341156\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.0003112937367700162\n",
            "training, acc: 0.999, loss: 0.002 lr: 0.0003112937367700162\n",
            "validation, acc: 0.974, loss: 0.157\n",
            "==============================================\n",
            "epoch: 60\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.000311284046692607\n",
            "step: 200, acc: 1.000, loss: 0.000 lr: 0.00030935808197989176\n",
            "step: 374, acc: 1.000, loss: 0.002 lr: 0.00030770177543924424\n",
            "training, acc: 1.000, loss: 0.001 lr: 0.00030770177543924424\n",
            "validation, acc: 0.974, loss: 0.160\n",
            "==============================================\n",
            "epoch: 61\n",
            "step: 0, acc: 1.000, loss: 0.001 lr: 0.0003076923076923077\n",
            "step: 200, acc: 1.000, loss: 0.000 lr: 0.0003058103975535168\n",
            "step: 374, acc: 1.000, loss: 0.000 lr: 0.0003041917624870718\n",
            "training, acc: 1.000, loss: 0.002 lr: 0.0003041917624870718\n",
            "validation, acc: 0.974, loss: 0.155\n",
            "==============================================\n",
            "epoch: 62\n",
            "step: 0, acc: 1.000, loss: 0.000 lr: 0.0003041825095057034\n",
            "step: 200, acc: 1.000, loss: 0.002 lr: 0.0003023431594860166\n",
            "step: 374, acc: 1.000, loss: 0.006 lr: 0.0003007609251406058\n",
            "training, acc: 0.999, loss: 0.003 lr: 0.0003007609251406058\n",
            "validation, acc: 0.971, loss: 0.176\n",
            "Evaluation on test set\n",
            "validation, acc: 0.971, loss: 0.175\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}